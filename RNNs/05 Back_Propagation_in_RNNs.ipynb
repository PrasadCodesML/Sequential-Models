{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Backpropagation in RNN\n",
        "\n",
        "Backpropagation in RNN is also known as backpropagation through time\n",
        "\n",
        "The backward pass propagates the error from the output at the final time step back through all earlier time steps. This is the essence of BPTT:\n",
        "\n",
        "Step 1: Loss Gradient at Each Time Step: Gradients are calculated for the loss at every time step with respect to the output, hidden states, and weights.\n",
        "\n",
        "Step 2: Chain Rule Application Over Time: Gradients are backpropagated not only through the layers but also through the time dimension. This considers how errors at later time steps (\n",
        "𝑡\n",
        "+\n",
        "1\n",
        ",\n",
        "𝑡\n",
        "+\n",
        "2\n",
        ") affect earlier time steps (\n",
        "𝑡\n",
        ").\n",
        "\n",
        "Key gradients to calculate include:\n",
        "\n",
        "Output weights (\n",
        "𝑊\n",
        "ℎ\n",
        "𝑦\n",
        "): Gradients depend on the relationship between hidden states and outputs.\n",
        "\n",
        "Hidden-to-Hidden Weights (\n",
        "𝑊\n",
        "ℎ\n",
        "ℎ\n",
        "): Gradients consider the feedback effect of a hidden state on its past versions.\n",
        "\n",
        "Input-to-Hidden Weights (\n",
        "𝑊\n",
        "𝑥\n",
        "ℎ\n",
        "): Gradients involve how inputs influence the hidden states."
      ],
      "metadata": {
        "id": "Pr5ob_OVKKSY"
      }
    }
  ]
}