{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### What are LSTMs\n",
        "\n",
        "The problem with RNNs was not having long context due to vanishing / exploading gradients\n",
        "\n",
        "RNN -> have state but it can only maintain short term context because there is only one state then this state have to pressure to keep the long term context and short term context and it fails to keep the long term context\n",
        "\n",
        "What if we maintain one more path to store long term memory\n",
        "\n",
        "LSTM -> here there are two state unlike RNN which are short term state and long term state so when ever a word is inputted it goes to the short term state and this short term state communicates with the long term state weather to put this word in long term state and when a new word comes then along with this word both the inputs long term state and short term state both will be inputted.\n",
        "\n",
        "In the architecture of LSTM there are 3 gates\n",
        "1. Forget Gate -> Based on the current input and the short term context it decides which things to remove from long term memory\n",
        "\n",
        "2. Input Gate -> Based on current input decides what new information should be added in long term memory\n",
        "\n",
        "3. Output Gate -> Based on current input what thing to remove from long term memory and show as output also at every given state this gate is the one to create short term memory for next input\n",
        "\n",
        "Input in LSTM -> We give 3 inputs in LSTM\n",
        "1. Short term memory of previous state at (t-1) --> h (t-1)\n",
        "2. Long term memory of previous state at (t-1) --> c (t-1)\n",
        "3. Input word\n",
        "\n",
        "Output in LSTM -> We recieve 2 outputs from LSTM\n",
        "1. Short term memory for next input --> h (t-1)\n",
        "2. Long term memory for next input --> c (t-1)\n",
        "\n",
        "What happens inside\n",
        "1. You update LTM\n",
        "2. You create STM"
      ],
      "metadata": {
        "id": "sUHbzzvX0R1B"
      }
    }
  ]
}